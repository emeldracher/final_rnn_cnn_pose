# -*- coding: utf-8 -*-
"""pr_skeleton_csv_download.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k2tDh8UF6w6r5dzp4xpHtpWZT8FdJtpB
"""

import tensorflow as tf
import tensorflow_hub as hub
import os


model = hub.load('https://bit.ly/metrabs_s')  # Takes about 3 minutes
! wget -q https://istvansarandi.com/eccv22_demo/test.jpg
img = tf.image.decode_image(tf.io.read_file('test.jpg'))
pred = model.detect_poses(img, skeleton='smpl+head_30')
pred['poses3d'].shape

joint_names = model.per_skeleton_joint_names['smpl+head_30'].numpy().astype(str)
joint_edges = model.per_skeleton_joint_edges['smpl+head_30'].numpy()
print(joint_names)

#############tracking biggest person adding none if not there comparing frame to frame##########
#############tracking biggest person adding none if not there comparing frame to frame##########
#############tracking biggest person adding none if not there comparing frame to frame##########
#############tracking biggest person adding none if not there comparing frame to frame##########

import tensorflow as tf
import cv2
import numpy as np
import imageio
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from mpl_toolkits.mplot3d import Axes3D
from scipy.spatial.distance import cdist

# Function to process a batch of frames
def process_batch(batch, model):
    batch = tf.stack(batch)
    pred = model.detect_poses_batched(batch, skeleton='smpl+head_30')
    return pred['poses2d'].numpy(), pred['poses3d'].numpy(), pred['boxes'].numpy()

# Function to calculate Euclidean distance between joints
def calculate_distance(joints1, joints2):
    return np.mean(np.linalg.norm(joints1 - joints2, axis=1))

# Function to calculate bounding box IoU
def calculate_iou(box1, box2):
    x1, y1, w1, h1 = box1[:4]  # Take only the first four values
    x2, y2, w2, h2 = box2[:4]  # Take only the first four values
    xi1 = max(x1, x2)
    yi1 = max(y1, y2)
    xi2 = min(x1 + w1, x2 + w2)
    yi2 = min(y1 + h1, y2 + h2)
    intersection = max(0, xi2 - xi1) * max(0, yi2 - yi1)
    union = w1 * h1 + w2 * h2 - intersection
    return intersection / union if union > 0 else 0

# Initialize video and parameters
video_path = '/content/pr1w_2023_ned_8.mov'  # The path you use in cv2.VideoCapture
cap = cv2.VideoCapture(video_path)
fpsog = cap.get(cv2.CAP_PROP_FPS)

# Extract video name from the path
video_name = os.path.basename(video_path).split('.')[0]  # Get base name without extension

batch_size = 32  # Adjust based on your GPU's memory capacity
resize_dim = (1280, 720)  # Resize frames to 720p for efficiency

original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
original_dimensions = (original_width, original_height)

frames = []
pose2d_results = []
pose3d_results = []
boxes_results = []

# Read and preprocess video frames
while True:
    ret, frame = cap.read()
    if not ret:
        break
    #frame_resized = cv2.resize(frame, resize_dim)  # Resize to reduce memory usage
    frame_resized = cv2.resize(frame, original_dimensions)  # Resize to reduce memory usage
    frames.append(frame_resized)  # Keep in uint8 format


cap.release()

# Process frames in batches
for i in range(0, len(frames), batch_size):
    batch = frames[i:i+batch_size]
    # Convert batch to TensorFlow tensors
    batch_tensors = [tf.convert_to_tensor(frame, dtype=tf.uint8) for frame in batch]
    pose2d_batch, pose3d_batch, boxes_batch = process_batch(batch_tensors, model)

    # Append results per frame
    pose2d_results.extend(pose2d_batch)  # Store 2D poses per frame
    pose3d_results.extend(pose3d_batch)  # Store 3D poses per frame
    boxes_results.extend(boxes_batch)  # Store boxes per frame

# Track the same person across frames
tracked_person_indices = []  # Store the tracked person's index for each frame
tracked_pose2d = []          # Store 2D pose data of the tracked person
tracked_pose3d = []          # Store 3D pose data of the tracked person

# Initialize tracking with the largest person in the first frame
reference_pose3d = None
reference_box = None
if len(pose3d_results[0]) > 0:
    largest_person_idx = max(
        range(len(boxes_results[0])),
        key=lambda idx: boxes_results[0][idx][2] * boxes_results[0][idx][3]
    )
    tracked_person_indices.append(largest_person_idx)
    tracked_pose2d.append(pose2d_results[0][largest_person_idx])
    tracked_pose3d.append(pose3d_results[0][largest_person_idx])
    reference_pose3d = pose3d_results[0][largest_person_idx]
    reference_box = boxes_results[0][largest_person_idx]

# Process subsequent frames
for frame_idx in range(1, len(pose3d_results)):
    if len(pose3d_results[frame_idx]) == 0 or reference_pose3d is None:
        # If no people are detected or no reference person, skip frame
        tracked_person_indices.append(None)
        tracked_pose2d.append(None)
        tracked_pose3d.append(None)
        continue

    # Find the best match for the reference person in the current frame
    min_distance = float('inf')
    best_match_idx = None
    for person_idx in range(len(pose3d_results[frame_idx])):
        pose3d = pose3d_results[frame_idx][person_idx]
        box = boxes_results[frame_idx][person_idx]

        # Calculate distance and IoU
        distance = calculate_distance(reference_pose3d, pose3d)
        iou = calculate_iou(reference_box, box)

        # Use a combination of proximity and bounding box overlap
        if distance < min_distance and iou > 0.1:  # IoU threshold to avoid switching
            min_distance = distance
            best_match_idx = person_idx

    if best_match_idx is not None:
        tracked_person_indices.append(best_match_idx)
        tracked_pose2d.append(pose2d_results[frame_idx][best_match_idx])
        tracked_pose3d.append(pose3d_results[frame_idx][best_match_idx])
        reference_pose3d = pose3d_results[frame_idx][best_match_idx]
        reference_box = boxes_results[frame_idx][best_match_idx]
    else:
        # Skip frame if no match is found
        tracked_person_indices.append(None)
        tracked_pose2d.append(None)
        tracked_pose3d.append(None)

#frame_width, frame_height = resize_dim  # Assuming resize_dim is (width, height)
frame_width, frame_height = original_dimensions  # Assuming resize_dim is (width, height)

# Generate 2D and 3D visualizations for the tracked person
gif_images = []
for i, frame in enumerate(frames):
    if tracked_person_indices[i] is None:
        continue  # Skip frames where no person is tracked

    fig = plt.figure(figsize=(15, 7))

    # Plot the original frame with 2D skeletons
    image_ax = fig.add_subplot(1, 2, 1)
    image_ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for Matplotlib

    image_ax.set_xlim(0, frame_width)
    image_ax.set_ylim(frame_height, 0)  # Invert Y-axis for correct orientation
    image_ax.axis('off')  # Turn off axis ticks for better visualization

    # Highlight the tracked person in red, others in green
    for idx, box in enumerate(boxes_results[i]):
        x, y, w, h = box[:4]
        rect_color = 'r' if idx == tracked_person_indices[i] else 'b'
        rect = Rectangle((x, y), w, h, fill=False, edgecolor=rect_color, linewidth=2)
        image_ax.add_patch(rect)

        pose2d = pose2d_results[i][idx]
        line_color = 'red' if idx == tracked_person_indices[i] else 'green'
        for i_start, i_end in joint_edges:
            image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), color=line_color, marker='o', markersize=2)

    # Plot the 3D skeleton
    pose_ax = fig.add_subplot(1, 2, 2, projection='3d')
    pose_ax.view_init(0, -90)
    pose_ax.set_xlim3d(-1500, 1500)
    pose_ax.set_zlim3d(-1500, 1500)
    pose_ax.set_ylim3d(-1500, 5000)

    pose3d_frame = pose3d_results[i]
    pose3d_frame[..., 1], pose3d_frame[..., 2] = pose3d_frame[..., 2], -pose3d_frame[..., 1]

    for person_idx in range(pose3d_frame.shape[0]):
        line_color = 'red' if person_idx == tracked_person_indices[i] else 'green'
        for i_start, i_end in joint_edges:
            pose_ax.plot(*zip(pose3d_frame[person_idx, i_start], pose3d_frame[person_idx, i_end]),
                         color=line_color, marker='o', markersize=2)

        pose_ax.text(pose3d_frame[person_idx, 0, 0],
                     pose3d_frame[person_idx, 0, 1],
                     pose3d_frame[person_idx, 0, 2],
                     f'{person_idx}', color='blue', fontsize=8, fontweight='bold')

    # Save the current figure as an image and add it to the gif_images list
    fig.canvas.draw()
    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')
    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))
    gif_images.append(image)
    plt.clf()
    plt.close(fig)

# Save the images as a video
output_path = '/content/a_pose_estimated_graphs.mp4'
imageio.mimwrite(output_path, gif_images, fps=fpsog, codec='libx264')

# Print debug information
print(f"Number of Frames Processed: {len(frames)}")
print(f"Original Video FPS: {fpsog}")

# Check if entire frames are missing
missing_frames = [idx for idx, frame in enumerate(tracked_pose3d) if frame is None]
print(f"Missing frames: {missing_frames}")

# Check for invalid joints in non-missing frames
for idx, frame in enumerate(tracked_pose3d):
    if frame is not None:
        for joint_idx, joint in enumerate(frame):
            if joint is None or not np.isfinite(joint).all():
                print(f"Invalid joint found at frame {idx}, joint {joint_idx}: {joint}")

# Extract joint 21 positions for the tracked person
#######KEEP IN MIND X AND Y AXIS WAS FLIPPED ABOVE!!!!!!!!!!
joint_positions_wrist = []  # To store the positions of joint 21 for the tracked person
joint_positions_knee = []
joint_positions_shoulder = []

for frame_idx in range(len(tracked_pose3d)):
    if tracked_pose3d[frame_idx] is not None:
        joint_positions_wrist.append(tracked_pose3d[frame_idx][21])  # Extract joint 21
        joint_positions_knee.append(tracked_pose3d[frame_idx][4])  # Extract joint 21
        joint_positions_shoulder.append(tracked_pose3d[frame_idx][16])  # Extract joint 21
    else:
        joint_positions_wrist.append(None)  # Append None for skipped frames

# Calculate velocity and acceleration for joint 21
joint_positions_filtered = np.array([pos for pos in joint_positions_wrist if pos is not None])


print(min(joint_positions_filtered[:, 0]))
print(max(joint_positions_filtered[:, 0]))

# Compute velocities as differences between consecutive positions
joint_velocities = np.diff(joint_positions_filtered, axis=0)

# Compute accelerations as differences between consecutive velocities
joint_accelerations = np.diff(joint_velocities, axis=0)

# Plot the X, Y, Z coordinates of joint 21 position BUT THEY WERE FLIIPED ABOVE
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(range(len(joint_positions_filtered)), joint_positions_filtered[:, 0], label='X Position', marker='o')
ax.plot(range(len(joint_positions_filtered)), joint_positions_filtered[:, 1], label='Y Position', marker='o')
ax.plot(range(len(joint_positions_filtered)), joint_positions_filtered[:, 2], label='Z Position', marker='o')
ax.set_title('Tracked Joint 21 Position Over Frames')
ax.set_xlabel('Frame')
ax.set_ylabel('Position')
ax.legend()
plt.grid()
plt.show()

# Plot the X, Y, Z components of velocity
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(range(len(joint_velocities)), joint_velocities[:, 0], label='X Velocity', marker='o')
ax.plot(range(len(joint_velocities)), joint_velocities[:, 1], label='Y Velocity', marker='o')
ax.plot(range(len(joint_velocities)), joint_velocities[:, 2], label='Z Velocity', marker='o')
ax.set_title('Tracked Joint 21 Velocity Over Frames')
ax.set_xlabel('Frame')
ax.set_ylabel('Velocity')
ax.legend()
plt.grid()
plt.show()

# Plot the X, Y, Z components of acceleration
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(range(len(joint_accelerations)), joint_accelerations[:, 0], label='X Acceleration', marker='o')
ax.plot(range(len(joint_accelerations)), joint_accelerations[:, 1], label='Y Acceleration', marker='o')
ax.plot(range(len(joint_accelerations)), joint_accelerations[:, 2], label='Z Acceleration', marker='o')
ax.set_title('Tracked Joint 21 Acceleration Over Frames')
ax.set_xlabel('Frame')
ax.set_ylabel('Acceleration')
ax.legend()
plt.grid()
plt.show()

from scipy.ndimage import gaussian_filter1d
import numpy as np
import matplotlib.pyplot as plt

# Function to compute velocity and acceleration, with smoothing applied only to velocity and acceleration
def process_joint_positions(joint_positions, fps, sigma=9):
    # Filter out None values
    joint_positions_filtered = np.array([pos for pos in joint_positions if pos is not None])

    # Compute velocities and accelerations
    velocities = np.diff(joint_positions_filtered, axis=0) * fps  # Convert to mm/s
    accelerations = np.diff(velocities, axis=0) * fps  # Convert to mm/s²

    # Smooth velocities and accelerations
    smoothed_velocities = gaussian_filter1d(velocities, sigma=sigma, axis=0)
    smoothed_accelerations = gaussian_filter1d(accelerations, sigma=sigma, axis=0)

    return joint_positions_filtered, smoothed_velocities, smoothed_accelerations

# Frame rate of the video (adjust as needed, e.g., 30 fps for typical video)
fps = fpsog  # Replace with your video's frame rate
time_positions = lambda frames: np.array(frames) / fps  # Converts frame indices to time in seconds

# Process wrist, knee, and shoulder joints
wrist_data = process_joint_positions(joint_positions_wrist, fps)
knee_data = process_joint_positions(joint_positions_knee, fps)
shoulder_data = process_joint_positions(joint_positions_shoulder, fps)

# Plot all X positions on the same graph (unsmoothed)
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time_positions(range(len(wrist_data[0]))), wrist_data[0][:, 0], label='Wrist X Position', marker='o')
ax.plot(time_positions(range(len(knee_data[0]))), knee_data[0][:, 0], label='Knee X Position', marker='o')
ax.plot(time_positions(range(len(shoulder_data[0]))), shoulder_data[0][:, 0], label='Shoulder X Position', marker='o')
ax.set_title('X Position of Wrist, Knee, and Shoulder Over Time (Unsmoothed)')
ax.set_xlabel('Time (s)')
ax.set_ylabel('Position (X) [mm]')
ax.legend()
plt.grid()
plt.show()

# Plot all X velocities on the same graph (smoothed)
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time_positions(range(len(wrist_data[1]))), wrist_data[1][:, 0], label='Wrist X Velocity (Smoothed)', marker='o')
ax.plot(time_positions(range(len(knee_data[1]))), knee_data[1][:, 0], label='Knee X Velocity (Smoothed)', marker='o')
ax.plot(time_positions(range(len(shoulder_data[1]))), shoulder_data[1][:, 0], label='Shoulder X Velocity (Smoothed)', marker='o')
ax.set_title('X Velocity of Wrist, Knee, and Shoulder Over Time (Smoothed)')
ax.set_xlabel('Time (s)')
ax.set_ylabel('Velocity (X) [mm/s]')
ax.legend()
plt.grid()
plt.show()

# Plot all X accelerations on the same graph (smoothed)
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time_positions(range(len(wrist_data[2]))), wrist_data[2][:, 0], label='Wrist X Acceleration (Smoothed)', marker='o')
ax.plot(time_positions(range(len(knee_data[2]))), knee_data[2][:, 0], label='Knee X Acceleration (Smoothed)', marker='o')
ax.plot(time_positions(range(len(shoulder_data[2]))), shoulder_data[2][:, 0], label='Shoulder X Acceleration (Smoothed)', marker='o')
ax.set_title('X Acceleration of Wrist, Knee, and Shoulder Over Time (Smoothed)')
ax.set_xlabel('Time (s)')
ax.set_ylabel('Acceleration (X) [mm/s²]')
ax.legend()
plt.grid()
plt.show()

print (len(wrist_data[0]))

##########same plot but interpolated if a frame is missing
from scipy.ndimage import gaussian_filter1d
from scipy.interpolate import interp1d
import numpy as np
import matplotlib.pyplot as plt

# Interpolate missing joint positions for all joints
def interpolate_all_joints(tracked_pose3d):
    """Interpolate all joints across all frames and count interpolations."""
    num_frames = len(tracked_pose3d)
    num_joints = len(tracked_pose3d[0]) if tracked_pose3d[0] is not None else 0
    interpolated_pose3d = np.zeros((num_frames, num_joints, 3))
    interpolation_count = 0

    for joint_idx in range(num_joints):
        joint_positions = [frame[joint_idx] if frame is not None else None for frame in tracked_pose3d]

        # Create mask and interpolate for each dimension (X, Y, Z)
        mask = np.array([pos is not None for pos in joint_positions])
        valid_indices = np.where(mask)[0]
        valid_positions = np.array([joint_positions[idx] for idx in valid_indices])

        if len(valid_indices) < num_frames:  # If there are missing frames, we are interpolating
            interpolation_count += num_frames - len(valid_indices)

        for dim in range(3):  # X, Y, Z
            f = interp1d(valid_indices, valid_positions[:, dim], kind='linear', fill_value="extrapolate")
            interpolated_pose3d[:, joint_idx, dim] = f(range(num_frames))

    print(f"Total interpolations performed: {interpolation_count}")
    return interpolated_pose3d

# Interpolate all joints and count interpolations
interpolated_pose3d = interpolate_all_joints(tracked_pose3d)

# Extract wrist, knee, and shoulder positions from the interpolated data
joint_positions_wrist = interpolated_pose3d[:, 21, :]
joint_positions_knee = interpolated_pose3d[:, 4, :]
joint_positions_shoulder = interpolated_pose3d[:, 16, :]

# Function to compute velocity and acceleration
def process_joint_positions(joint_positions, fps, sigma=9):
    # Compute velocities and accelerations
    velocities = np.diff(joint_positions, axis=0) * fps  # Convert to mm/s
    accelerations = np.diff(velocities, axis=0) * fps  # Convert to mm/s²

    # Smooth velocities and accelerations
    smoothed_velocities = gaussian_filter1d(velocities, sigma=sigma, axis=0)
    smoothed_accelerations = gaussian_filter1d(accelerations, sigma=sigma, axis=0)

    return joint_positions, smoothed_velocities, smoothed_accelerations

# Frame rate of the video
fps = fpsog  # Replace with your video's frame rate
time_positions = lambda frames: np.array(frames) / fps  # Converts frame indices to time in seconds

# Process wrist, knee, and shoulder joints
wrist_data = process_joint_positions(joint_positions_wrist, fps)
knee_data = process_joint_positions(joint_positions_knee, fps)
shoulder_data = process_joint_positions(joint_positions_shoulder, fps)

# Plot all X positions on the same graph (unsmoothed)
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time_positions(range(len(wrist_data[0]))), wrist_data[0][:, 0], label='Wrist X Position', marker='o')
ax.plot(time_positions(range(len(knee_data[0]))), knee_data[0][:, 0], label='Knee X Position', marker='o')
ax.plot(time_positions(range(len(shoulder_data[0]))), shoulder_data[0][:, 0], label='Shoulder X Position', marker='o')
ax.set_title('X Position of Wrist, Knee, and Shoulder Over Time (Unsmoothed)')
ax.set_xlabel('Time (s)')
ax.set_ylabel('Position (X) [mm]')
ax.legend()
plt.grid()
plt.show()

# Plot all X velocities on the same graph (smoothed)
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time_positions(range(len(wrist_data[1]))), wrist_data[1][:, 0], label='Wrist X Velocity (Smoothed)', marker='o')
ax.plot(time_positions(range(len(knee_data[1]))), knee_data[1][:, 0], label='Knee X Velocity (Smoothed)', marker='o')
ax.plot(time_positions(range(len(shoulder_data[1]))), shoulder_data[1][:, 0], label='Shoulder X Velocity (Smoothed)', marker='o')
ax.set_title('X Velocity of Wrist, Knee, and Shoulder Over Time (Smoothed)')
ax.set_xlabel('Time (s)')
ax.set_ylabel('Velocity (X) [mm/s]')
ax.legend()
plt.grid()
plt.show()

# Plot all X accelerations on the same graph (smoothed)
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time_positions(range(len(wrist_data[2]))), wrist_data[2][:, 0], label='Wrist X Acceleration (Smoothed)', marker='o')
ax.plot(time_positions(range(len(knee_data[2]))), knee_data[2][:, 0], label='Knee X Acceleration (Smoothed)', marker='o')
ax.plot(time_positions(range(len(shoulder_data[2]))), shoulder_data[2][:, 0], label='Shoulder X Acceleration (Smoothed)', marker='o')
ax.set_title('X Acceleration of Wrist, Knee, and Shoulder Over Time (Smoothed)')
ax.set_xlabel('Time (s)')
ax.set_ylabel('Acceleration (X) [mm/s²]')
ax.legend()
plt.grid()
plt.show()

import numpy as np
import plotly.graph_objects as go
from scipy.ndimage import gaussian_filter1d

# Function to process joint positions and compute velocity and acceleration
def process_joint_positions(joint_positions, fps, sigma=9):
    joint_positions_filtered = np.array([pos for pos in joint_positions if pos is not None])

    # Compute velocities and accelerations
    velocities = np.diff(joint_positions_filtered, axis=0) * fps  # Convert to mm/s
    accelerations = np.diff(velocities, axis=0) * fps  # Convert to mm/s²

    # Smooth velocities and accelerations
    smoothed_velocities = gaussian_filter1d(velocities, sigma=sigma, axis=0)
    smoothed_accelerations = gaussian_filter1d(accelerations, sigma=sigma, axis=0)

    return joint_positions_filtered, smoothed_velocities, smoothed_accelerations

# Frame rate of the video
fps = fpsog  # Replace with your video's frame rate
time_positions = lambda frames: np.array(frames) / fps  # Converts frame indices to time in seconds

# Centering function
def center_on_pelvis(tracked_pose3d, pelvis_index=0):
    """Translate all joints so the pelvis is always at (0, 0, 0)."""
    centered_pose3d = []
    for frame in tracked_pose3d:
        if frame is not None:
            pelvis_position = frame[pelvis_index]  # Get pelvis position for the frame
            frame_centered = frame - pelvis_position  # Subtract pelvis position
            centered_pose3d.append(frame_centered)
        else:
            centered_pose3d.append(None)  # Keep None for skipped frames
    return centered_pose3d

# Center the tracked pose data on the pelvis
pelvis_index = 0  # Replace with the correct index for the pelvis joint in your dataset
tracked_pose3d_centered = center_on_pelvis(interpolated_pose3d, pelvis_index)

# Update joint positions for wrist, knee, and shoulder after centering
joint_positions_wrist = [frame[21] if frame is not None else None for frame in tracked_pose3d_centered]
joint_positions_knee = [frame[4] if frame is not None else None for frame in tracked_pose3d_centered]
joint_positions_shoulder = [frame[16] if frame is not None else None for frame in tracked_pose3d_centered]

# Process wrist, knee, and shoulder joints
wrist_data = process_joint_positions(joint_positions_wrist, fps)
knee_data = process_joint_positions(joint_positions_knee, fps)
shoulder_data = process_joint_positions(joint_positions_shoulder, fps)

# Time array
time = time_positions(range(len(wrist_data[0])))

# Create the Plotly figure for positions
fig = go.Figure()

# Add X positions
fig.add_trace(go.Scatter(x=time, y=wrist_data[0][:, 0], mode='lines+markers', name='Wrist X Position',
                         line=dict(color='crimson')))
fig.add_trace(go.Scatter(x=time, y=knee_data[0][:, 0], mode='lines+markers', name='Knee X Position',
                         line=dict(color='lightskyblue')))
fig.add_trace(go.Scatter(x=time, y=shoulder_data[0][:, 0], mode='lines+markers', name='Shoulder X Position',
                         line=dict(color='midnightblue')))

# Update layout for positions
fig.update_layout(
    title='X Position of Wrist, Knee, and Shoulder Over Time (Centered on Pelvis)',
    xaxis_title='Time (s)',
    yaxis_title='Position (X) [mm]',
    legend=dict(orientation="h", x=0.5, xanchor="center", y=-0.2),
    template='plotly_white'
)
fig.show()

# Create the Plotly figure for velocities
fig = go.Figure()
fig.add_trace(go.Scatter(x=time[:-1], y=-wrist_data[1][:, 0], mode='lines+markers', name='Wrist X Velocity',
                         line=dict(color='crimson')))
fig.add_trace(go.Scatter(x=time[:-1], y=-knee_data[1][:, 0], mode='lines+markers', name='Knee X Velocity',
                         line=dict(color='lightskyblue')))
fig.add_trace(go.Scatter(x=time[:-1], y=-shoulder_data[1][:, 0], mode='lines+markers', name='Shoulder X Velocity',
                         line=dict(color='midnightblue')))

# Update layout for velocities
fig.update_layout(
    title='X Velocity of Wrist, Knee, and Shoulder Over Time (Centered on Pelvis)',
    xaxis_title='Time (s)',
    yaxis_title='Velocity (X) [mm/s]',
    legend=dict(orientation="h", x=0.5, xanchor="center", y=-0.2),
    template='plotly_white'
)
fig.show()

# Create the Plotly figure for accelerations
fig = go.Figure()
fig.add_trace(go.Scatter(x=time[:-2], y=-wrist_data[2][:, 0], mode='lines+markers', name='Wrist X Acceleration',
                         line=dict(color='crimson')))
fig.add_trace(go.Scatter(x=time[:-2], y=-knee_data[2][:, 0], mode='lines+markers', name='Knee X Acceleration',
                         line=dict(color='lightskyblue')))
fig.add_trace(go.Scatter(x=time[:-2], y=-shoulder_data[2][:, 0], mode='lines+markers', name='Shoulder X Acceleration',
                         line=dict(color='midnightblue')))

# Update layout for accelerations
fig.update_layout(
    title='X Acceleration of Wrist, Knee, and Shoulder Over Time (Centered on Pelvis)',
    xaxis_title='Time (s)',
    yaxis_title='Acceleration (X) [mm/s²]',
    legend=dict(orientation="h", x=0.5, xanchor="center", y=-0.2),
    template='plotly_white'
)
fig.show()

print(min(wrist_data[0][:, 0]))
print(max(wrist_data[0][:, 0]))

import numpy as np
import plotly.graph_objects as go
import plotly.subplots as sp
from scipy.ndimage import gaussian_filter1d

# Function to calculate angle between three points
def calculate_angle(A, B, C):
    """
    Calculate the angle (in degrees) between three points A, B, and C.
    Args:
        A (ndarray): Coordinates of the first point.
        B (ndarray): Coordinates of the second point (vertex of the angle).
        C (ndarray): Coordinates of the third point.
    Returns:
        float: Angle in degrees.
    """
    AB = B - A
    AC = C - A
    dot_product = np.dot(AB, AC)
    magnitude_AB = np.linalg.norm(AB)
    magnitude_AC = np.linalg.norm(AC)
    cos_theta = dot_product / (magnitude_AB * magnitude_AC)
    angle_rad = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # Clip for numerical stability
    angle_deg = np.degrees(angle_rad)
    return angle_deg

# Indices for joints
pelvis_index = 3  # Index for pelvis
left_shoulder_index = 16  # Index for left shoulder
left_elbow_index = 18  # Index for left elbow

# Frame rate of the video
fps =  fpsog # Replace with your video's frame rate


# Calculate original shoulder angle (Pelvis, Elbow, Shoulder)
shoulder_angles = []
for frame in tracked_pose3d_centered:
    if frame is not None:
        # Extract joint positions
        pelvis = frame[pelvis_index]
        left_shoulder = frame[left_shoulder_index]
        left_elbow = frame[left_elbow_index]

        # Correct joint coordinates if Y and Z axes are flipped
        pelvis_corrected = np.array([pelvis[0], pelvis[2], -pelvis[1]])
        left_shoulder_corrected = np.array([left_shoulder[0], left_shoulder[2], -left_shoulder[1]])
        left_elbow_corrected = np.array([left_elbow[0], left_elbow[2], -left_elbow[1]])

        # Calculate angle
        angle = calculate_angle(left_shoulder_corrected, left_elbow_corrected, pelvis_corrected)
        shoulder_angles.append(angle)
    else:
        shoulder_angles.append(None)

# Smooth the original shoulder angles
shoulder_angles_filled = np.array([angle if angle is not None else np.nan for angle in shoulder_angles])
smoothed_angles = gaussian_filter1d(
    np.nan_to_num(shoulder_angles_filled, nan=np.nanmean(shoulder_angles_filled)), sigma=7
)

# Calculate new shoulder angle to the positive x-axis
left_shoulder_angles_x = []
for frame in tracked_pose3d_centered:
    if frame is not None:
        # Extract joint positions
        pelvis = frame[pelvis_index]
        left_shoulder = frame[left_shoulder_index]

        # Correct joint coordinates if Y and Z axes are flipped
        pelvis_corrected = np.array([pelvis[0], pelvis[2], -pelvis[1]])
        left_shoulder_corrected = np.array([left_shoulder[0], left_shoulder[2], -left_shoulder[1]])

        # Define the positive x-axis reference point relative to the shoulder
        positive_x_axis = np.array([left_shoulder_corrected[0] + 1, left_shoulder_corrected[1], left_shoulder_corrected[2]])

        # Calculate the angle
        angle_to_x_axis = calculate_angle(left_shoulder_corrected, pelvis_corrected, positive_x_axis)
        left_shoulder_angles_x.append(angle_to_x_axis)
    else:
        left_shoulder_angles_x.append(None)

# Smooth the new shoulder angles
left_shoulder_angles_x_filled = np.array([angle if angle is not None else np.nan for angle in left_shoulder_angles_x])
smoothed_left_shoulder_angles_x = gaussian_filter1d(
    np.nan_to_num(left_shoulder_angles_x_filled, nan=np.nanmean(left_shoulder_angles_x_filled)), sigma=7
)

# Calculate trunk angle relative to the positive x-axis
trunk_angles_x = []
for frame in tracked_pose3d_centered:
    if frame is not None:
        # Extract joint positions
        pelvis = frame[pelvis_index]
        left_shoulder = frame[left_shoulder_index]

        # Correct joint coordinates if Y and Z axes are flipped
        pelvis_corrected = np.array([pelvis[0], pelvis[2], -pelvis[1]])
        left_shoulder_corrected = np.array([left_shoulder[0], left_shoulder[2], -left_shoulder[1]])

        # Calculate the trunk vector (from pelvis to shoulder)
        trunk_vector = left_shoulder_corrected - pelvis_corrected

        # Define the reference vector along the positive x-axis
        positive_x_axis = np.array([1, 0, 0])

        # Calculate the angle between the trunk vector and the positive x-axis
        dot_product = np.dot(trunk_vector, positive_x_axis)
        magnitude_trunk = np.linalg.norm(trunk_vector)
        magnitude_x_axis = np.linalg.norm(positive_x_axis)
        cos_theta = dot_product / (magnitude_trunk * magnitude_x_axis)
        angle_rad = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # Clip for numerical stability
        angle_deg = np.degrees(angle_rad)

        # Append the calculated angle
        trunk_angles_x.append(angle_deg)
    else:
        # Append None for missing frames
        trunk_angles_x.append(None)

trunk_angles_x_filled = np.array([angle if angle is not None else np.nan for angle in trunk_angles_x])
smoothed_pelvis_angles_x = gaussian_filter1d(
    np.nan_to_num(trunk_angles_x_filled, nan=np.nanmean(trunk_angles_x_filled)), sigma=7
)

# Time array for plotting
time_steps = np.arange(len(shoulder_angles)) / fps

# Create subplots
fig = sp.make_subplots(
    rows=3, cols=1,  # Two rows, one column
    shared_xaxes=True,  # Share the x-axis
    vertical_spacing=0.15,  # Space between subplots
    subplot_titles=(
        "Shoulder Angle Over Time (Pelvis, Elbow, Shoulder)",
        "Shoulder Angle to Positive X-Axis Over Time",
        "Trunk Angle to Positive X-Axis Over Time"
    )
)

# Subplot 1: Original Shoulder Angle
fig.add_trace(
    go.Scatter(
        x=time_steps,
        y=shoulder_angles,
        mode='markers',
        name='Original Shoulder Angle',
        marker=dict(size=6, color='RoyalBlue', opacity=0.5)
    ),
    row=1, col=1
)
fig.add_trace(
    go.Scatter(
        x=time_steps,
        y=smoothed_angles,
        mode='lines',
        name='Smoothed Original Shoulder Angle',
        line=dict(width=2, color='DarkBlue')
    ),
    row=1, col=1
)

# Subplot 2: Shoulder Angle to Positive X-Axis
fig.add_trace(
    go.Scatter(
        x=time_steps,
        y=left_shoulder_angles_x,
        mode='markers',
        name='Shoulder Angle to X-Axis',
        marker=dict(size=6, color='Crimson', opacity=0.5)
    ),
    row=2, col=1
)
fig.add_trace(
    go.Scatter(
        x=time_steps,
        y=smoothed_left_shoulder_angles_x,
        mode='lines',
        name='Smoothed Shoulder Angle to X-Axis',
        line=dict(width=2, color='DarkRed')
    ),
    row=2, col=1
)

# Update layout
fig.update_layout(
    height=1200,  # Adjust height for better visibility with three subplots
    title_text="Angles Over Time",
    xaxis_title="Time (seconds)",
    yaxis_title="Angle (degrees)",
    showlegend=True,
    template='plotly_white'
)

# Subplot 3: Pelvis Angle to Positive X-Axis
fig.add_trace(
    go.Scatter(
        x=time_steps,
        y=trunk_angles_x,
        mode='markers',
        name='Pelvis Angle to X-Axis',
        marker=dict(size=6, color='Green', opacity=0.5)
    ),
    row=3, col=1
)
fig.add_trace(
    go.Scatter(
        x=time_steps,
        y=smoothed_pelvis_angles_x,
        mode='lines',
        name='Smoothed Pelvis Angle to X-Axis',
        line=dict(width=2, color='DarkGreen')
    ),
    row=3, col=1
)

# Adjust subplot-specific axis titles
fig['layout']['yaxis1']['title'] = 'Angle (degrees)'
fig['layout']['yaxis2']['title'] = 'Angle (degrees)'
fig['layout']['yaxis3']['title'] = 'Angle (degrees)'
fig['layout']['xaxis3']['title'] = 'Time (seconds)'

# Show the figure
fig.show()

import pandas as pd
import numpy as np

# Assuming the following are already computed:
# `time`, `wrist_data`, `knee_data`, `shoulder_data`, `shoulder_angles`,
# `left_shoulder_angles_x`, `trunk_angles_x`, `tracked_pose3d_centered`,
# `fps`, `video_name`, `pr_label`, `joint_names`

# Prepare DataFrame for CSV output
data = {
    "Label": ["pr1"] * len(time),
    "Video": [video_name] * len(time),
    "Frame": np.arange(len(time)),
    "Time": time,
    "Shoulder_Angle": shoulder_angles,  # Original shoulder angle
    "Shoulder_Angle_to_X": left_shoulder_angles_x,  # New shoulder angle to x-axis
    "Trunk_Angle_to_X": trunk_angles_x,  # Trunk angle to x-axis
}

# Add joint XYZ coordinates using the joint names
for joint_idx, joint_name in enumerate(joint_names):
    for axis, label in zip(range(3), ["X", "Y", "Z"]):
        data[f"{joint_name}_{label}"] = [
            frame[joint_idx, axis] if frame is not None else None
            for frame in tracked_pose3d_centered
        ]

# Add velocities and accelerations for wrist, shoulder, and knee
for joint_name, joint_data in zip(
    ['lwri_smpl', 'lsho_smpl', 'lkne_smpl'],  # Use provided names for wrist, shoulder, knee
    [wrist_data, shoulder_data, knee_data]
):
    for axis, label in zip(range(3), ["X", "Y", "Z"]):
        data[f"{joint_name}_Vel_{label}"] = np.pad(
            joint_data[1][:, axis], (1, 0), constant_values=np.nan
        )  # Velocity
        data[f"{joint_name}_Acc_{label}"] = np.pad(
            joint_data[2][:, axis], (2, 0), constant_values=np.nan
        )  # Acceleration

# Create DataFrame
df = pd.DataFrame(data)

# Save DataFrame to CSV
output_csv_path = f"{video_name}_processed_data.csv"
df.to_csv(output_csv_path, index=False)

print(f"Data successfully saved to {output_csv_path}.")

from scipy.signal import savgol_filter
import plotly.graph_objects as go
import numpy as np

# Function to smooth a trajectory while preserving overall distance
def smooth_trajectory_preserving_distance(joint_positions, window_length=50, polyorder=4):
    """Smooth a 2D trajectory (X/Y) while preserving overall distance."""
    smoothed_positions = savgol_filter(joint_positions, window_length=window_length, polyorder=polyorder, axis=0)
    return smoothed_positions

# Extract the X/Y trajectory of the wrist (raw)
wrist_positions_xy = wrist_data[0][:, :3]  # Extract X and Y positions

# Apply smoothing while preserving distance
wrist_positions_xy_smoothed = smooth_trajectory_preserving_distance(wrist_positions_xy)

# Generate time values
time = time_positions(range(len(wrist_positions_xy)))  # Time in seconds

# Create a color scale based on time
colorscale = time

# Create the Plotly figure
fig = go.Figure()

# Add the smoothed trajectory with color changing over time
fig.add_trace(go.Scatter(
    x=wrist_positions_xy_smoothed[:, 0],
    y=wrist_positions_xy_smoothed[:, 2],
    mode='markers+lines',
    marker=dict(
        size=8,
        color=colorscale,  # Color based on time
        colorscale='balance',  # Choose a colormap
        colorbar=dict(title="Time (s)")
    ),
    line=dict(
        color='rgba(0,0,0,0.2)'  # Optional faint line for clarity
    ),
    #name='Smoothed Wrist Trajectory'
))

# Add raw trajectory for comparison
#fig.add_trace(go.Scatter(
    #x=wrist_positions_xy[:, 0],
   # y=wrist_positions_xy[:, 2],
   # mode='markers+lines',
    #marker=dict(
     #   size=5,
       # color='red',  # Red color for raw data
   # ),
   # line=dict(
      #  color='rgba(255,0,0,0.2)'  # Optional faint line for clarity
   # ),
    #name='Raw Wrist Trajectory'
#))

# Set titles and layout
fig.update_layout(
    title="Wrist Movement in X/Z Direction (Centered on Pelvis, Smoothed)",
    xaxis_title="X Position [mm]",
    yaxis_title="Z Position [mm]",
    showlegend=True,
    template="plotly_white",
    height=600,
    width=1200
)

# Show the plot
fig.show()

########MOVIE OF CENTERED SKELETON#############
########MOVIE OF CENTERED SKELETON#############
########MOVIE OF CENTERED SKELETON#############
########MOVIE OF CENTERED SKELETON#############
########MOVIE OF CENTERED SKELETON#############

import matplotlib.pyplot as plt
import numpy as np
import imageio



# Indices for joints
pelvis_index = 3  # Index for pelvis
left_shoulder_index = 16  # Index for left shoulder
left_elbow_index = 18  # Index for left elbow

# List to store all images for the video
gif_images = []

# Frame rate of the output video
fpsog = 60  # Adjust as needed

# Loop through all frames in the centered pose data
for i, frame in enumerate(tracked_pose3d_centered):
    if frame is None:
        continue  # Skip frames with no data

    # Calculate the shoulder angle
    pelvis = frame[pelvis_index]
    left_shoulder = frame[left_shoulder_index]
    left_elbow = frame[left_elbow_index]
    shoulder_angle = calculate_angle(left_shoulder, left_elbow, pelvis)

    # Create a new figure
    fig = plt.figure(figsize=(15, 7))

    # Plot the 3D skeleton
    pose_ax = fig.add_subplot(1, 1, 1, projection='3d')
    pose_ax.view_init(0, -90)

    # Set axis limits
    pose_ax.set_xlim3d(-1500, 1500)
    pose_ax.set_zlim3d(-1500, 1500)
    pose_ax.set_ylim3d(-1500, 1500)

    # Plot joints and edges
    for i_start, i_end in joint_edges:
        if i_start < frame.shape[0] and i_end < frame.shape[0]:
            pose_ax.plot(
                [frame[i_start, 0], frame[i_end, 0]],
                [frame[i_start, 1], frame[i_end, 1]],
                [frame[i_start, 2], frame[i_end, 2]],
                color='green',
                marker='o',
                markersize=2,
            )

    # Annotate the shoulder angle
    pose_ax.text(
        left_shoulder[0], left_shoulder[1], left_shoulder[2] + 500,  # Offset above the joint
        f'{shoulder_angle:.1f}°',
        color='blue',
        fontsize=10,
        weight='bold'
    )

    # Save the current figure as an image
    fig.canvas.draw()
    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')
    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))
    gif_images.append(image)
    plt.clf()
    plt.close(fig)

# Save the images as a video

output_path = f'/content/{video_name}_pose_animation_with_angles.mp4'
imageio.mimwrite(output_path, gif_images, fps=fpsog, codec='libx264')

print(f"Video saved to {output_path}")