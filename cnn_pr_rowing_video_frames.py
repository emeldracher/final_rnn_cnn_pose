# -*- coding: utf-8 -*-
"""cnn_pr_rowing_video_frames.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T6Ik9VgMgwyM_r9jZPK0_xZWj_ycDFh4
"""

import os

# Define the path to your zip file
zip_file_path = "/content/rowingvids.zip"

# Unzip the file
!unzip -q "{zip_file_path}" -d "/content"

# List the extracted contents
print("Extracted folders:")
print(os.listdir("/content"))
print("Contents of PR1 folder:")
print(os.listdir("/content/rowingvids/pr1_men"))

print("\nContents of PR2 folder:")
print(os.listdir("/content/rowingvids/pr2_men"))

def extract_frames_from_folder(video_folder, label, output_dir):
    """
    Extract frames from all videos in a folder and save them to a labeled directory structure.
    """
    # Include .mov in the list of supported formats
    videos = [os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith(('.mp4', '.avi', '.mov'))]
    print(f"Found {len(videos)} videos in {video_folder}")

    for video_path in videos:
        print(f"Processing video: {video_path}")
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        label_dir = os.path.join(output_dir, label, video_name)
        os.makedirs(label_dir, exist_ok=True)

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"Error: Cannot open video {video_path}")
            continue

        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_path = os.path.join(label_dir, f"frame_{frame_count}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_count += 1

        cap.release()
        print(f"Extracted {frame_count} frames from {video_name}")

# Define paths

import cv2

video_root = "/content/rowingvids"  # Root folder where "pr1_og" and "pr2_og" are located
output_root = "/content/dataset"  # Destination folder for extracted frames

# Extract frames for PR1 and PR2
extract_frames_from_folder(os.path.join(video_root, "pr1_men"), "pr1", output_root)
extract_frames_from_folder(os.path.join(video_root, "pr2_men"), "pr2", output_root)
extract_frames_from_folder(os.path.join(video_root, "pr1_women"), "pr1", output_root)
extract_frames_from_folder(os.path.join(video_root, "pr2_women"), "pr2", output_root)

import os
import shutil
import random

def shuffle_frames_across_videos(source_dir, output_dir):
    """
    Randomly shuffle frames across all videos and save them in a single directory per class.

    Parameters:
    - source_dir: Path to the source dataset directory (structured by class/video/frame).
    - output_dir: Path to the output directory for shuffled frames.
    """
    os.makedirs(output_dir, exist_ok=True)

    for label in os.listdir(source_dir):
        label_path = os.path.join(source_dir, label)
        if not os.path.isdir(label_path):
            continue

        # Create output directory for the label
        output_label_dir = os.path.join(output_dir, label)
        os.makedirs(output_label_dir, exist_ok=True)

        # Gather all frames under this label
        all_frames = []
        for video in os.listdir(label_path):
            video_path = os.path.join(label_path, video)
            if os.path.isdir(video_path):
                all_frames += [os.path.join(video_path, frame) for frame in os.listdir(video_path) if frame.endswith(('.jpg', '.png'))]

        # Shuffle frames
        random.shuffle(all_frames)

        # Save shuffled frames into the output directory
        for i, frame_path in enumerate(all_frames):
            new_frame_path = os.path.join(output_label_dir, f"frame_{i}.jpg")
            shutil.copy(frame_path, new_frame_path)
        print(f"Shuffled and saved {len(all_frames)} frames for label: {label}")

# Source and output directories
source_dir = "/content/dataset"  # Original dataset structured by class/video/frame
output_dir = "/content/shuffled_dataset"  # New dataset directory with shuffled frames

# Shuffle frames across videos
shuffle_frames_across_videos(source_dir, output_dir)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define image data generators for training data
datagen_train = ImageDataGenerator(
    rescale=1.0 / 255.0,  # Normalize pixel values
    validation_split=0.2  # Split data into training and validation
)

# Define image data generators for testing data with horizontal flip
datagen_test = ImageDataGenerator(
    rescale=1.0 / 255.0,  # Normalize pixel values
    horizontal_flip=True,  # Apply horizontal flip
    validation_split=0.2  # Split data into training and validation
)

# Load training data (no flipping)
train_data = datagen_train.flow_from_directory(
    "/content/shuffled_dataset",
    target_size=(224, 224),
    batch_size=32,
    class_mode="binary",
    subset="training",
    shuffle=True  # Explicitly enable shuffling
)

# Load validation data with horizontal flipping
val_data = datagen_test.flow_from_directory(
    "/content/shuffled_dataset",
    target_size=(224, 224),
    batch_size=32,
    class_mode="binary",
    subset="validation"
)

import matplotlib.pyplot as plt

# Check the first 5 images from the validation data
for i in range(5):
    images, labels = next(val_data)  # Fetch a batch
    image = images[0]  # Select the first image in the batch
    label = labels[0]  # Corresponding label

    # Plot the image
    plt.figure()
    plt.imshow(image)
    plt.title(f"Image {i+1}: {'pr1' if label == 0 else 'pr2'}")
    plt.axis("off")
    plt.show()

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

# Load the pre-trained ResNet50 model without the top layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model
base_model.trainable = False

# Add custom layers
model = Sequential([
    base_model,
    Flatten(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train the custom layers
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=5,  # Train for a few epochs initially
    batch_size=32
)

# Unfreeze the base model
base_model.trainable = True

# Freeze all layers except the last 50 layers of ResNet50
for layer in base_model.layers[:-50]:
    layer.trainable = False

# Recompile the model with a lower learning rate
from tensorflow.keras.optimizers import Adam
model.compile(
    optimizer=Adam(learning_rate=0.0001),  # Lower learning rate for fine-tuning
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Fine-tune the model
history_fine_tune = model.fit(
    train_data,
    validation_data=val_data,
    epochs=5,  # Fine-tune for more epochs
    batch_size=32
)

# Save the fine-tuned model
model.save("/content/resnet50_fine_tuned.h5")

test_loss, test_accuracy = model.evaluate(val_data)
print(f"Test Accuracy: {test_accuracy}")

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Load the saved model
model = tf.keras.models.load_model("/content/resnet50_fine_tuned.h5")


# Preprocess validation sample
val_sample = val_data[0][0][0]  # Single image from validation set
val_sample_array = np.expand_dims(val_sample, axis=0)  # Add batch dimension

def compute_saliency_map(model, img_array):
    img_array = tf.convert_to_tensor(img_array)
    with tf.GradientTape() as tape:
        tape.watch(img_array)
        predictions = model(img_array)  # Get model predictions
        loss = predictions[:, 0]  # Use the single output for binary classification

    gradients = tape.gradient(loss, img_array)  # Compute gradients
    saliency = tf.reduce_max(tf.abs(gradients), axis=-1)[0]  # Take max over color channels
    saliency = np.maximum(saliency, 0) / np.max(saliency)  # Normalize between 0 and 1
    return saliency  # Return as-is, no need for .numpy()

# Compute saliency map
saliency = compute_saliency_map(model, val_sample_array)


# Plot the saliency map
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(val_sample / 255.0)
plt.title("Original Image")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(saliency, cmap="hot")
plt.title("Saliency Map")
plt.axis("off")
plt.show()

# Retrieve the name of the image from validation data
image_index = 0  # Index of the image you are using (adjust as needed)
image_name = val_data.filenames[image_index]

print(f"Image Name: {image_name}")

def compute_saliency_map_enhanced(model, img_array):
    img_array = tf.convert_to_tensor(img_array)
    with tf.GradientTape() as tape:
        tape.watch(img_array)
        predictions = model(img_array)
        loss = predictions[:, 0]  # Binary classification: use the single output
        print(f"Loss: {loss.numpy()}")  # Debugging

    gradients = tape.gradient(loss, img_array)
    print(f"Gradients: {gradients}")  # Debugging

    # Combine gradients into saliency map
    saliency = tf.reduce_max(tf.abs(gradients), axis=-1)[0]

    # Amplify saliency values
    saliency = saliency * 10  # Amplify by a factor of 10
    saliency = np.clip(saliency, 0, 1)  # Ensure values are between 0 and 1

    print(f"Saliency Map (Enhanced): {saliency}")  # Debugging
    return saliency

# Compute enhanced saliency map
saliency_enhanced = compute_saliency_map_enhanced(model, val_sample_array)

# Plot the enhanced saliency map
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(val_sample / 255.0)
plt.title("Original Image")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(saliency_enhanced, cmap="magma")  # Use a different color map
plt.title("Enhanced Saliency Map")
plt.axis("off")
plt.show()

raw_image, label = next(val_data)  # Get a batch of images and labels
original_image = raw_image[0]  # Select the first image in the batch

# Display the original image
plt.figure(figsize=(5, 5))
plt.imshow(original_image)
plt.title(f"Original Image (Label: {'pr1' if label[0] == 0 else 'pr2'})")
plt.axis("off")
plt.show()

import matplotlib.pyplot as plt

# Retrieve the raw image and compute saliency map
raw_image, label = next(val_data)  # Get a batch of images and labels
original_image = raw_image[0]  # Select the first image in the batch
original_image_array = np.expand_dims(original_image, axis=0)  # Add batch dimension for model input

# Compute saliency map
saliency = compute_saliency_map_enhanced(model, original_image_array)

# Plot the original image and saliency map side-by-side
plt.figure(figsize=(10, 5))

# Plot original image
plt.subplot(1, 2, 1)
plt.imshow(original_image)
plt.title(f"Original Image (Label: {'pr1' if label[0] == 0 else 'pr2'})")
plt.axis("off")

# Plot saliency map
plt.subplot(1, 2, 2)
plt.imshow(saliency, cmap="magma")  # Use a visually striking color map
plt.title("Saliency Map")
plt.axis("off")

plt.show()

import numpy as np

# Predict on validation data
predictions = model.predict(val_data)
predicted_labels = (predictions > 0.5).astype(int).flatten()  # Convert probabilities to binary labels
true_labels = val_data.classes  # True labels from the generator
class_indices = {v: k for k, v in val_data.class_indices.items()}  # Map indices to class names

# Find misclassified indices
misclassified_indices = np.where(predicted_labels != true_labels)[0]

# Initialize counters
pr1_as_pr2 = 0
pr2_as_pr1 = 0
pr1_as_pr2_videos = []
pr2_as_pr1_videos = []

# Get file paths and labels of misclassified images
misclassified_files = [val_data.filepaths[i] for i in misclassified_indices]

for i, idx in enumerate(misclassified_indices):
    true_label = true_labels[idx]
    predicted_label = predicted_labels[idx]
    video_name = misclassified_files[i].split("/")[-2]  # Assuming filepaths are like .../pr1/video/frame.jpg

    if true_label == 0 and predicted_label == 1:  # PR1 mistaken as PR2
        pr1_as_pr2 += 1
        pr1_as_pr2_videos.append(video_name)
    elif true_label == 1 and predicted_label == 0:  # PR2 mistaken as PR1
        pr2_as_pr1 += 1
        pr2_as_pr1_videos.append(video_name)

# Print results
print(f"PR1 mistaken as PR2: {pr1_as_pr2} times")
print("Videos where PR1 was mistaken as PR2:")
print(set(pr1_as_pr2_videos))

print(f"\nPR2 mistaken as PR1: {pr2_as_pr1} times")
print("Videos where PR2 was mistaken as PR1:")
print(set(pr2_as_pr1_videos))

plt.imshow(val_sample.astype("uint8"))  # Display the raw image without normalization
plt.title("Original Raw Image")
plt.axis("off")
plt.show()

import matplotlib.pyplot as plt

# Get file paths of misclassified images
misclassified_files = [val_data.filepaths[i] for i in misclassified_indices]

# Plot misclassified images
plt.figure(figsize=(12, 12))
for i, file_path in enumerate(misclassified_files[:36]):  # Display up to 16 misclassified images
    img = plt.imread(file_path)
    true_label = class_indices[true_labels[misclassified_indices[i]]]
    predicted_label = class_indices[predicted_labels[misclassified_indices[i]]]
    plt.subplot(10, 10, i + 1)
    plt.imshow(img)
    plt.axis('off')
    plt.title(f"True: {true_label}\nPred: {predicted_label}")
plt.tight_layout()
plt.show()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define image data generator for training (no flipping)
datagen_train = ImageDataGenerator(
    rescale=1.0 / 255.0,  # Normalize pixel values
    validation_split=0.2  # Split data into training and validation
)

# Define image data generator for validation (with horizontal flipping)
datagen_val = ImageDataGenerator(
    rescale=1.0 / 255.0,  # Normalize pixel values
    horizontal_flip=True,  # Apply horizontal flipping
    validation_split=0.2  # Split data into training and validation
)

# Load training data (no flipping)
train_data = datagen_train.flow_from_directory(
    "/content/shuffled_dataset",  # Path to dataset
    target_size=(128, 128),  # Resize images to a smaller size for faster training
    batch_size=32,
    class_mode="binary",
    subset="training",
    shuffle=True  # Shuffle training data
)

# Load validation data (flipped horizontally)
val_data = datagen_val.flow_from_directory(
    "/content/shuffled_dataset",
    target_size=(128, 128),  # Same size as training data
    batch_size=32,
    class_mode="binary",
    subset="validation"
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Define the CNN architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),  # First convolutional layer
    MaxPooling2D(pool_size=(2, 2)),  # First max pooling layer
    Conv2D(64, (3, 3), activation='relu'),  # Second convolutional layer
    MaxPooling2D(pool_size=(2, 2)),  # Second max pooling layer
    Conv2D(128, (3, 3), activation='relu'),  # Third convolutional layer
    MaxPooling2D(pool_size=(2, 2)),  # Third max pooling layer
    Flatten(),  # Flatten the 3D feature maps into a 1D feature vector
    Dense(128, activation='relu'),  # Fully connected layer
    Dropout(0.5),  # Dropout for regularization
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(
    optimizer='adam',  # Adam optimizer
    loss='binary_crossentropy',  # Binary classification loss
    metrics=['accuracy']  # Track accuracy during training
)

# Display the model summary
model.summary()

# Train the model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=4,  # Number of training epochs
    batch_size=32,
    verbose=1  # Display training progress
)

# Save the trained model
model.save("/content/pr1_pr2_classifier.h5")

# Evaluate the model
val_loss, val_accuracy = model.evaluate(val_data)
print(f"Validation Loss: {val_loss}")
print(f"Validation Accuracy: {val_accuracy}")

import numpy as np

# Predict on validation data
predictions = model.predict(val_data)
predicted_labels = (predictions > 0.5).astype(int).flatten()  # Convert probabilities to binary labels
true_labels = val_data.classes  # True labels from the generator
class_indices = {v: k for k, v in val_data.class_indices.items()}  # Map indices to class names

# Find misclassified indices
misclassified_indices = np.where(predicted_labels != true_labels)[0]

# Initialize counters
pr1_as_pr2 = 0
pr2_as_pr1 = 0
pr1_as_pr2_files = []
pr2_as_pr1_files = []

# Get file paths and labels of misclassified images
misclassified_files = [val_data.filepaths[i] for i in misclassified_indices]

for i, idx in enumerate(misclassified_indices):
    true_label = true_labels[idx]
    predicted_label = predicted_labels[idx]
    filepath = misclassified_files[i]

    if true_label == 0 and predicted_label == 1:  # PR1 mistaken as PR2
        pr1_as_pr2 += 1
        pr1_as_pr2_files.append(filepath)
    elif true_label == 1 and predicted_label == 0:  # PR2 mistaken as PR1
        pr2_as_pr1 += 1
        pr2_as_pr1_files.append(filepath)

# Print results
print(f"PR1 mistaken as PR2: {pr1_as_pr2} times")
print("Files where PR1 was mistaken as PR2:")
for file in pr1_as_pr2_files:
    print(file)

print(f"\nPR2 mistaken as PR1: {pr2_as_pr1} times")
print("Files where PR2 was mistaken as PR1:")
for file in pr2_as_pr1_files:
    print(file)

