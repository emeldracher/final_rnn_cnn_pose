# -*- coding: utf-8 -*-
"""NOT TRUNCATEDDrun_csv_data_pr1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oRnFlDjp8oAxk3kp5d95pl11_L9sERSM
"""

!pip install tensorflow

import os
import pandas as pd
import glob

# Directory containing the 26 CSV files
csv_directory = '/content/'

# Load all CSV files in the directory
csv_files = glob.glob(os.path.join(csv_directory, "*.csv"))

# Placeholder for all processed videos
processed_videos = {}

# Process each CSV file
for file in csv_files:
    # Read CSV file
    df = pd.read_csv(file)

    # Ensure the DataFrame is sorted by Frame or Time
    df = df.sort_values(by='Frame').reset_index(drop=True)

    # Fill NaN values in acceleration and velocity columns
    velocity_cols = [col for col in df.columns if 'Vel' in col]
    acceleration_cols = [col for col in df.columns if 'Acc' in col]
    fill_cols = velocity_cols + acceleration_cols

    for col in fill_cols:
        if col in df.columns:
            df[col] = df[col].fillna(0)  # Fill NaN with 0

    # Get video name from file name (or another column, if available)
    video_name = os.path.basename(file).replace(".csv", "")

    # Save the processed DataFrame with the video name
    processed_videos[video_name] = df

# Combine all processed DataFrames for a single combined CSV if needed
combined_df = pd.concat(processed_videos.values(), ignore_index=True)

# Save the combined DataFrame to a CSV for verification
output_path = '/content/untruncated_combined_variable_length_data.csv'
combined_df.to_csv(output_path, index=False)

# Print video names for verification
print("Processed Videos:", list(processed_videos.keys()))

# Load the uploaded combined variable-length data
untruncated_file_path = '/content/untruncated_combined_variable_length_data.csv'
untruncated_data = pd.read_csv(untruncated_file_path)

# Display the structure of the loaded data
untruncated_data.info(), untruncated_data.head()

# Group data by 'Video'
grouped_data = untruncated_data.groupby('Video')

sequences = []
labels = []
video_train = []  # Initialize video_train

for video, group in grouped_data:
    # Drop non-numerical columns
    features = group.drop(columns=['Label', 'Video', 'Frame', 'Time', 'lkne_smpl_X','lkne_smpl_Y','lkne_smpl_Z', 'rkne_smpl_X', 'rkne_smpl_Y', 'rkne_smpl_Z'
    , 'lank_smpl_X','lank_smpl_Y','lank_smpl_Z','rank_smpl_X', 'rank_smpl_Y', 'rank_smpl_Z', 'ltoe_smpl_X', 'ltoe_smpl_Y', 'ltoe_smpl_Z',
          'rtoe_smpl_X', 'rtoe_smpl_Y', 'rtoe_smpl_Z'])

    # Normalize the features (per sequence)
    features = (features - features.min()) / (features.max() - features.min() + 1e-8)

    # Append the sequence
    sequences.append(features.values)

    # Append the label (assuming all rows in a group have the same label)
    label = 0 if group['Label'].iloc[0] == 'pr1' else 1
    labels.append(label)

    # Append the video identifier
    video_train.append(video)

# Identify video names for each label
pr1_videos = [video for video, group in grouped_data if group['Label'].iloc[0] == 'pr1']
pr2_videos = [video for video, group in grouped_data if group['Label'].iloc[0] == 'pr2']

# Print video names under each label
print("Videos labeled as PR1:")
print(pr1_videos)
print(len(pr1_videos))

print("\nVideos labeled as PR2:")
print(pr2_videos)
print(len(pr2_videos))

print(len(video_train))



from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Convert video_train to a NumPy array
video_train = np.array(video_train)

# Pad sequences to the length of the longest sequence in the dataset
padded_sequences = pad_sequences(sequences, padding='post', dtype='float32')

# Split into train and test sets (including video_train)
X_train, X_test, y_train, y_test, video_train_train, video_train_test = train_test_split(
    padded_sequences, labels, video_train, test_size=0.2, random_state=42, stratify=labels
)

# Convert labels to NumPy arrays
y_train = np.array(y_train)
y_test = np.array(y_test)

# Compute class weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights_dict = dict(enumerate(class_weights))

# Output class weights for verification
print("Class Weights:", class_weights_dict)

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd

# Group and process data by video
grouped_data = untruncated_data.groupby('Video')

sequences = []
labels = []
video_train = []

# Define excluded headers for flipping
excluded_headers = ['trunk_angle_to_x', 'shoulder_angle_to_x']

# Identify columns with 'X' at the end but not in excluded headers
columns_to_flip = [
    col for col in untruncated_data.columns
    if col.endswith('_X') and col.lower() not in excluded_headers
]

# Process grouped data
for video, group in grouped_data:
    # Drop non-numerical columns
    features = group.drop(columns=['Label', 'Video', 'Frame', 'Time', 'lkne_smpl_X','lkne_smpl_Y','lkne_smpl_Z', 'rkne_smpl_X', 'rkne_smpl_Y', 'rkne_smpl_Z'
    , 'lank_smpl_X','lank_smpl_Y','lank_smpl_Z','rank_smpl_X', 'rank_smpl_Y', 'rank_smpl_Z', 'ltoe_smpl_X', 'ltoe_smpl_Y', 'ltoe_smpl_Z',
          'rtoe_smpl_X', 'rtoe_smpl_Y', 'rtoe_smpl_Z'])

    # Normalize the features (per sequence)
    features = (features - features.min()) / (features.max() - features.min() + 1e-8)

    # Append the original sequence
    sequences.append(features.values)

    # Append the label (assuming all rows in a group have the same label)
    label = 0 if group['Label'].iloc[0] == 'pr1' else 1
    labels.append(label)

    # Append the video identifier
    video_train.append(video)

# Pad sequences to the length of the longest sequence in the dataset
padded_sequences = pad_sequences(sequences, padding='post', dtype='float32')

# Split into train and test sets (including video_train)
X_train, X_test, y_train, y_test, video_train_train, video_train_test = train_test_split(
    padded_sequences, labels, video_train, test_size=0.2, random_state=42, stratify=labels
)

# Convert labels to NumPy arrays
y_train = np.array(y_train)
y_test = np.array(y_test)

# Flip the X-coordinates for the training dataset and duplicate it
X_train_flipped = []

for sequence in X_train:
    flipped_sequence = sequence.copy()
    for col_idx, col in enumerate(columns_to_flip):
        if col_idx < flipped_sequence.shape[1]:  # Ensure column index is within bounds
            flipped_sequence[:, col_idx] *= -1  # Flip the X values
    X_train_flipped.append(flipped_sequence)

# Scale the X-coordinates slightly up or down and duplicate the training dataset
X_train_scaled = []

scaling_factors = [0.9, 1.1]  # Slightly scale down and up
for sequence in X_train:
    for scale in scaling_factors:
        scaled_sequence = sequence.copy()
        for col_idx, col in enumerate(columns_to_flip):
            if col_idx < scaled_sequence.shape[1]:  # Ensure column index is within bounds
                scaled_sequence[:, col_idx] *= scale  # Scale the X values
        X_train_scaled.append(scaled_sequence)

# Combine original, flipped, and scaled training data
X_train_combined = np.concatenate((X_train, np.array(X_train_flipped), np.array(X_train_scaled)), axis=0)
y_train_combined = np.concatenate((y_train, y_train, np.tile(y_train, len(scaling_factors))), axis=0)

# Compute class weights for the combined training data
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_combined),
    y=y_train_combined
)
class_weights_dict = dict(enumerate(class_weights))

# Test on the non-flipped original coordinates
X_test_original = X_test  # Keep the original test dataset unchanged

# Output class weights for verification
print("Class Weights:", class_weights_dict)

# Print dataset shapes for verification
print(f"X_train shape: {X_train_combined.shape}")
print(f"X_test shape: {X_test_original.shape}")
print(f"y_train shape: {y_train_combined.shape}")
print(f"y_test shape: {y_test.shape}")

# Output the flipped columns for verification
print(f"Columns flipped: {columns_to_flip}")

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"video_train shape: {len(video_train_train)}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, Callback
from tensorflow.keras import Input
import numpy as np




# Custom Callback to Monitor Training Failures
class MonitorTrainingFailures(Callback):
    def __init__(self, X_train, y_train, video_train):
        super().__init__()
        self.X_train = X_train
        self.y_train = y_train
        self.video_train = video_train

    def on_epoch_end(self, epoch, logs=None):
        # Get predictions for the training data
        predictions = self.model.predict(self.X_train, verbose=0)
        predicted_labels = (predictions > 0.5).astype(int).flatten()

        # Identify failed predictions
        errors = predicted_labels != self.y_train

        # Log failed video IDs
        failed_videos = self.video_train[errors]
        print(f"\nEpoch {epoch + 1}: Failed on {len(failed_videos)} videos")
        print(f"Failed Video IDs: {failed_videos}")



# Build the RNN
#model = Sequential([
    #Input(shape=(None, X_train.shape[2])),  # Explicitly define input shape
    #Masking(mask_value=0.0),  # Masking layer
    #LSTM(32, return_sequences=False, activation='tanh'),  # Single LSTM layer
    #Dropout(0.5),  # Dropout for regularization
    #Dense(1, activation='sigmoid')  # Output layer for binary classification
#])

model = Sequential([
    Input(shape=(None, X_train.shape[2])),
    Masking(mask_value=0.0),
    Bidirectional(LSTM(32, return_sequences=False, activation='tanh')),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=0.0001),  # Slower learning rate for stable convergence
    loss='binary_crossentropy',  # Binary classification loss
    metrics=['accuracy']
)

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Create the failure monitoring callback
monitor_failures = MonitorTrainingFailures(X_train, y_train, np.array(video_train_train))


# Train the model
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=8,
    validation_split=0.2,  # Use part of training data for validation
    class_weight=class_weights_dict,  # Apply class weights
    callbacks=[early_stopping, monitor_failures],  # Early stopping and failure monitoring
    shuffle=False,  # Prevent shuffling to maintain alignment
    verbose=1
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {test_accuracy}")

import tensorflow as tf
import numpy as np

# Example test sample: Select one sequence from your test set
sample_idx = 0  # Index of the test sample to analyze
sample = X_test[sample_idx:sample_idx + 1]  # Shape: (1, time_steps, features)

# Ensure the sample is a TensorFlow tensor
sample = tf.convert_to_tensor(sample, dtype=tf.float32)

# Use GradientTape to compute gradients
with tf.GradientTape() as tape:
    tape.watch(sample)  # Watch the input sample
    prediction = model(sample)  # Forward pass through the model

# Calculate the gradients of the prediction w.r.t. the input features
grads = tape.gradient(prediction, sample)  # Shape: (1, time_steps, features)

# Aggregate saliency values across time steps for sequential data
saliency_values = tf.reduce_mean(tf.abs(grads), axis=1).numpy().flatten()

# Print the saliency values
print("Saliency Values for Features:", saliency_values)

# Drop non-numerical columns
dropped_features = df.drop(columns=['Label', 'Video', 'Frame', 'Time', 'lkne_smpl_X','lkne_smpl_Y','lkne_smpl_Z', 'rkne_smpl_X', 'rkne_smpl_Y', 'rkne_smpl_Z'
    , 'lank_smpl_X','lank_smpl_Y','lank_smpl_Z','rank_smpl_X', 'rank_smpl_Y', 'rank_smpl_Z', 'ltoe_smpl_X', 'ltoe_smpl_Y', 'ltoe_smpl_Z',
          'rtoe_smpl_X', 'rtoe_smpl_Y', 'rtoe_smpl_Z'])

# Get the remaining column names and their indices
remaining_columns = list(dropped_features.columns)

# Use the actual feature names for the saliency map
feature_names = remaining_columns  # Update the feature names dynamically

# Plotly code for the saliency map
import plotly.graph_objects as go
import numpy as np


# Create a Plotly bar chart with updated feature names
fig = go.Figure(data=[go.Bar(x=feature_names, y=saliency_values)])

fig.update_layout(
    title="Feature Importance (Saliency Map)",
    xaxis_title="Feature",
    yaxis_title="Importance",
    template="plotly_white"
)

fig.show()

dropped_features = df.drop(columns=['Label', 'Video', 'Frame', 'Time'])

# Get the remaining column names and their indices
remaining_columns = list(enumerate(dropped_features.columns))

# Print the column names and their indices
for index, column in remaining_columns:
    print(f"Index {index}: {column}")

# Assuming you have video identifiers for all sequences
video_ids = np.array([group_name for group_name, group in grouped_data])

# Split video identifiers alongside the data
X_train, X_test, y_train, y_test, video_train, video_test = train_test_split(
    padded_sequences, labels, video_ids, test_size=0.2, random_state=42, stratify=labels
)

# Get predictions
predictions = model.predict(X_test)

# Convert probabilities to binary predictions
predicted_labels = (predictions > 0.5).astype(int).flatten()

# Compare predictions with actual labels
errors = predicted_labels != y_test

# Map failed predictions back to videos
failed_videos = video_test[errors]

# Print results
print(f"Failed Predictions: {sum(errors)} / {len(y_test)}")
print("Failed Videos:", failed_videos)

import plotly.graph_objects as go

# Group data by 'Video'
grouped = combined_df.groupby('Video')

# Create a plotly figure
fig = go.Figure()

# Add a line trace for each video
for video, group in grouped:
    fig.add_trace(go.Scatter(
        x=group['Frame'],  # Frame as the x-axis
        y=group['Shoulder_Angle_to_X'],  # Shoulder_Angle_to_X as the y-axis
        mode='lines',
        name=str(video)  # Use the video name as the trace label
    ))

# Update layout for better visualization
fig.update_layout(
    title="Shoulder Angle to X for All Videos",
    xaxis_title="Frame",
    yaxis_title="Shoulder_Angle_to_X",
    legend_title="Video",
    template="plotly_white"
)

# Display the plot
fig.show()

from plotly.subplots import make_subplots
import plotly.graph_objects as go

# Filter data explicitly to avoid misclassification
pr1_data = untruncated_data[untruncated_data['Label'] == 'pr1']
pr2_data = untruncated_data[untruncated_data['Label'] == 'pr2']

# Debugging: Ensure no overlap
print("PR1 Videos:", pr1_data['Video'].unique())
print("PR2 Videos:", pr2_data['Video'].unique())

# Group videos by class
pr1_grouped = pr1_data.groupby('Video')
pr2_grouped = pr2_data.groupby('Video')

# Create subplots: one for PR1 and one for PR2
fig = make_subplots(
    rows=2, cols=1,
    shared_xaxes=True,
    vertical_spacing=0.1,
    subplot_titles=["PR1 Videos", "PR2 Videos"]
)

# Add a line trace for each PR1 video
for video, group in pr1_grouped:
    group = group.sort_values(by='Frame')  # Sort frames to avoid jumps
    fig.add_trace(
        go.Scatter(
            x=group['Frame'],
            y=group['Trunk_Angle_to_X'],
            mode='lines+markers',  # Include markers to distinguish gaps
            name=str(video),
            legendgroup="PR1",  # Group traces for PR1
        ),
        row=1, col=1
    )

# Add a line trace for each PR2 video
for video, group in pr2_grouped:
    group = group.sort_values(by='Frame')  # Sort frames to avoid jumps
    fig.add_trace(
        go.Scatter(
            x=group['Frame'],
            y=group['Trunk_Angle_to_X'],
            mode='lines+markers',  # Include markers to distinguish gaps
            name=str(video),
            legendgroup="PR2",  # Group traces for PR2
        ),
        row=2, col=1
    )

# Update layout
fig.update_layout(
    height=800,  # Adjust height for better readability
    title="Shoulder Angle to X for PR1 and PR2 Videos",
    xaxis_title="Frame",
    yaxis_title="Trunk_Angle_to_X",
    legend_title="Video",
    template="plotly_white",
    showlegend=True
)

# Show the plot
fig.show()

###########AMPLITUDE
# Filter data explicitly to avoid misclassification
pr1_data = untruncated_data[untruncated_data['Label'] == 'pr1']
pr2_data = untruncated_data[untruncated_data['Label'] == 'pr2']

# Calculate amplitude (max - min) for each video
pr1_amplitudes = pr1_data.groupby('Video')['Trunk_Angle_to_X'].agg(lambda x: x.max() - x.min())
pr2_amplitudes = pr2_data.groupby('Video')['Trunk_Angle_to_X'].agg(lambda x: x.max() - x.min())

# Rank PR1 and PR2 videos by amplitude
pr1_ranked = pr1_amplitudes.sort_values(ascending=False)
pr2_ranked = pr2_amplitudes.sort_values(ascending=False)

# Display ranked videos for PR1 and PR2
print("PR1 Videos Ranked by Amplitude:")
print(pr1_ranked)

print("\nPR2 Videos Ranked by Amplitude:")
print(pr2_ranked)

# Calculate the lowest angle for each video
pr1_lowest_angle = pr1_data.groupby('Video')['Trunk_Angle_to_X'].min()
pr2_lowest_angle = pr2_data.groupby('Video')['Trunk_Angle_to_X'].min()

# Rank PR1 and PR2 videos by the lowest angle
pr1_ranked_lowest = pr1_lowest_angle.sort_values()
pr2_ranked_lowest = pr2_lowest_angle.sort_values()

# Display ranked videos for PR1 and PR2 by lowest angle
print("PR1 Videos Ranked by Lowest Angle:")
print(pr1_ranked_lowest)

print("\nPR2 Videos Ranked by Lowest Angle:")
print(pr2_ranked_lowest)

# Calculate the highest angle for each video
pr1_highest_angle = pr1_data.groupby('Video')['Trunk_Angle_to_X'].max()
pr2_highest_angle = pr2_data.groupby('Video')['Trunk_Angle_to_X'].max()

# Rank PR1 and PR2 videos by the highest angle
pr1_ranked_highest = pr1_highest_angle.sort_values(ascending=False)
pr2_ranked_highest = pr2_highest_angle.sort_values(ascending=False)

# Display ranked videos for PR1 and PR2 by highest angle
print("PR1 Videos Ranked by Highest Angle:")
print(pr1_ranked_highest)

print("\nPR2 Videos Ranked by Highest Angle:")
print(pr2_ranked_highest)